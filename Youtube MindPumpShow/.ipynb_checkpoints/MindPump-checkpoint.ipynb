{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numeric-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('API.txt') as f:\n",
    "    lines = f.readlines()\n",
    "API_KEY = lines[0]\n",
    "CHANNEL_ID = \"UCq0hKkwnW5Cw1wQqu455WrA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_videos = \"https://www.googleapis.com/youtube/v3/search?key=\"+API_KEY+\"&channelId=\"+CHANNEL_ID+\"&part=snippet,id&order=date&maxResults=3000\"\n",
    "response = requests.get(url_videos).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-valve",
   "metadata": {},
   "source": [
    "Looking through the json file, all the relevant information is under the name items so I will pull \n",
    "only that to be used in our database.\n",
    "\n",
    "Now looking individually into what this search API call gives us, I'm going to take the first/most recent video and pull some data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alien-inspection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#searchResult',\n",
       " 'etag': 'hNZibq3nSrZ4aogxT-AMKGhJn98',\n",
       " 'id': {'kind': 'youtube#video', 'videoId': '0V_B20DqkT0'},\n",
       " 'snippet': {'publishedAt': '2022-03-19T00:27:03Z',\n",
       "  'channelId': 'UCq0hKkwnW5Cw1wQqu455WrA',\n",
       "  'title': 'What to Do After a Successful Reverse Diet, &amp; More (Listener Live Coaching) - 1774',\n",
       "  'description': '00:00 MAPS Aesthetic Giveaway 02:25 Mind Pump Fit Tip: You want to get a bigger bench press? GET STRONGER at the ...',\n",
       "  'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/0V_B20DqkT0/default.jpg',\n",
       "    'width': 120,\n",
       "    'height': 90},\n",
       "   'medium': {'url': 'https://i.ytimg.com/vi/0V_B20DqkT0/mqdefault.jpg',\n",
       "    'width': 320,\n",
       "    'height': 180},\n",
       "   'high': {'url': 'https://i.ytimg.com/vi/0V_B20DqkT0/hqdefault.jpg',\n",
       "    'width': 480,\n",
       "    'height': 360}},\n",
       "  'channelTitle': 'Mind Pump Show',\n",
       "  'liveBroadcastContent': 'none',\n",
       "  'publishTime': '2022-03-19T00:27:03Z'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In JSON files we can call them by name of the properties it has, therefore after\n",
    "# taking items we can take only the first item by indexing a 0\n",
    "response['items'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-desperate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0V_B20DqkT0 What to Do After a Successful Reverse Diet, &amp; More (Listener Live Coaching) - 1774 2022-03-19\n"
     ]
    }
   ],
   "source": [
    "print(response['items'][0]['id']['videoId'],\n",
    "response['items'][0]['snippet']['title'], \n",
    "response['items'][0]['snippet']['publishedAt'].split(\"T\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-space",
   "metadata": {},
   "source": [
    "So what we want is the videoID first so that we can pinpoint each video and find some values to \n",
    "analyze per video.\n",
    "\n",
    "Next we want the title of the actual video to check for clickbait and controversial views to be compared the values we will obtain per video.\n",
    "\n",
    "FInally, I want to include the date of each video just so we have a reference point to see growth of the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "connected-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in response['items']:\n",
    "    ids = i['id']['videoId']\n",
    "    titles = i['snippet']['title']\n",
    "    dates = i['snippet']['publishedAt'].split(\"T\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sublime-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a7_pf5CgRko'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-thailand",
   "metadata": {},
   "source": [
    "Now all the basic information that is needed from the videos is gathered, I want to start finding the specific details of each video. The initial API used was for searching for videos, now I want to gather the data for each video which means I need to use a different API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liquid-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_metrics = \"https://www.googleapis.com/youtube/v3/videos?id=\"+ids+\"&part=statistics&key=\"+API_KEY\n",
    "response_metrics = requests.get(url_metrics).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "living-banana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2117 85 4\n"
     ]
    }
   ],
   "source": [
    "print(response_metrics['items'][0]['statistics']['viewCount'],\n",
    "response_metrics['items'][0]['statistics']['likeCount'],\n",
    "response_metrics['items'][0]['statistics']['commentCount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-tunisia",
   "metadata": {},
   "source": [
    "Now to make a dataframe to tie all the data together, I want a function that can help gather information maybe not just restricted to this channnel. Majority of the work is complete we know which lines will output what, \n",
    "All that needs to be done is: \n",
    "    1. Create a dataframe\n",
    "    2. Place data in dataframe\n",
    "    3. Transfer to a csv\n",
    "\n",
    "We'll leave it open with the channel_id so if in the future another analysis on a youtube channnel is required I have majority of the stats that I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "answering-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information(channel_id):\n",
    "    pageToken = \"\"\n",
    "    # Setting up dataframe\n",
    "    categories = (\"ID\", \"Title\", \"Date\", \"Views\", \"Likes\", \"Comments\")\n",
    "    df = pd.DataFrame(columns=categories)\n",
    "    while 1:\n",
    "        url_videos = \"https://www.googleapis.com/youtube/v3/search?key=\"+API_KEY+\"&channelId=\"+channel_id+\"&part=snippet,id&order=date&maxResults=10000&\"+pageToken\n",
    "        response = requests.get(url_videos).json()\n",
    "        # In case the JSON file isn't fully rendered before going to loop\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Looping through JSON pulling the values needed in each video\n",
    "        for i in response['items']:\n",
    "            # Using our initial call to find basic information of videos\n",
    "            ids = i['id']['videoId']\n",
    "            title = i['snippet']['title']\n",
    "            Date = i['snippet']['publishedAt'].split(\"T\")[0]\n",
    "            \n",
    "            # Second API call to find statstics, using the information gathered in first call\n",
    "            url_metrics = \"https://www.googleapis.com/youtube/v3/videos?id=\"+ids+\"&part=statistics&key=\"+API_KEY\n",
    "            response_metrics = requests.get(url_metrics).json()\n",
    "        \n",
    "            views = response_metrics['items'][0]['statistics']['viewCount']\n",
    "            likes = response_metrics['items'][0]['statistics']['likeCount']\n",
    "            comments = response_metrics['items'][0]['statistics']['commentCount']\n",
    "            \n",
    "            #Filtering the data into corresponding column in dataframe\n",
    "            df = df.append({\"ID\": ids, \"Title\" : title, \"Date\":Date, \"Views\" : views, \"Likes\" : likes,\n",
    "                       \"Comments\" : comments}, ignore_index=True)\n",
    "        \n",
    "        # Need to ensure we go through the different pages given, otherwise our data is stuck at 50.\n",
    "        try:\n",
    "            if response['nextPageToken'] != None: \n",
    "                pageToken = \"pageToken=\" + response['nextPageToken']\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generous-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = get_information(\"UCq0hKkwnW5Cw1wQqu455WrA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exposed-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3018"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-demonstration",
   "metadata": {},
   "source": [
    "They have around 1700 podcasts, that include video, on this date however they also have specific sections for controversial topics, Q&A sections etc. So each upload of the podcast is accompanied by around 4-5 extra smaller clips. Not including other uploads of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accredited-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_csv(\"MindPumpStats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-inflation",
   "metadata": {},
   "source": [
    "Now we have all the data from MindPumps Youtube start all the way to March 19, 2022. \n",
    "And since we have the data in a csv file already, we will create a new notebook that has the analysis in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-allowance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
